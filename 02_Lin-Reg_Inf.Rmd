---
title: 'Linear Regression: Inference'
date: "2023/6/22"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


> Learning points:
>
> * package `wooldridge` for all the dataset of wooldridge's book
> * use `data()` function to load the dataset from the package
> * calculate critical value for *t-test* using `qt()` function
> * conduct *F-test* using `linearHypothesis()` function in `car` package
> * calculate confidence interval using `confint()` function in R `stats` package


# Multivariate OLS Regression

```{r}
# load the gpa1 dataset from the package wooldridge
library(wooldridge)
data("gpa1")
head(gpa1, 3)
```

We are interested in the following multivariate regression
$$ colGPA_i = \beta_0 + \beta_1 hsGPA_i + \beta_2 ACT_i + \beta_3 skipped_i + \epsilon_i $$

```{r}
# run OLS regression
multivariate_OLS <- lm(colGPA ~ hsGPA + ACT + skipped, data=gpa1)
summary(multivariate_OLS)
```

## Testing a Single Hypothesis

Notice that next to the coefficients, R automatically displays the standard errors associated with the respective coefficient _under the assumption of homoskedasticity._ Next to the standard error, you see the t-statistic associated with each coefficient. Lastly, R also displays the p-values.

*Null hypothesis*

We are interested in knowing whether the coefficient on `skipped` is statistically significantly different from zero. The null hypothesis in this case is
$$\beta_3 = 0$$
The two-sided alternative hypothesis in this case is
$$\beta_3 \neq 0$$ 


*t-statistic*

One way to test whether a coefficient is statistically significantly different from zero is to conduct a *t-test*. As we now know, R automatically calculates the t-statistics for us. In this case, we find a t-statistic of $-3.20$. Note that you can calculate this statistic yourself: $(-.083 - 0)/.026 = -3.20$. We now want to compare this value with some critical value and R also allows us to compute the critical value as follows

```{r}
# critical value for two-sided test
p = .05
df = 141 - 4
qt(p/2, df, lower.tail = FALSE)
```

The third argument `lower.tail = FALSE` tells R that we are interested in the "probability to the right" of *p*. Notice that since we are doing a two-tailed test, if you wrote `lower.tail = TRUE` you would get the same critical value, just with a minus in front. 

Given this critical value, we can now compare the absolute value of our t-statistic (3.2) with 1.977. Clearly, $3.20 > 1.977$, and hence we conclude that our estimate of $-.083$ is statistically significantly different from zero at the 5% significance level and reject the null hypothesis stated above.

In case you were testing a one-sided alternative hypothesis (instead of a two-sided one), we would need to find a different critical value. We can do this in R as follows
```{r eval=FALSE}
# critical value for one-sided test
qt(p, df, lower.tail=FALSE)
```

*p-values*

R computed the *p-values* associated with the coefficients. By comparing the calculated *p-value* with 0.05, we reject the null hypothesis if the p-value is less than 0.05 (at the 5% level). 

The *p-value* from a *F-test* as well as the one from a *t-test* are the same when testing a single hypothesis. We have above seen that the *p-value* from the *t-test* is $0.00173$. To conduct a *F-test* by the `linearHypothesis()` function in the `car` package as follow:

```{r, message=FALSE}
# conduct F-test
library(car)
linearHypothesis(multivariate_OLS, c("skipped = 0"))
```

*Confidence intervals*

The 95% confidence interval for the coefficient on `skipped` should be approximately $[-.083 \pm 1.96*0.026] = [-.134,-.032]$. We could ask R to compute this for us as follows

```{r}
# calculate confidence interval
confint(multivariate_OLS, 'skipped', level=0.95)
```

*A new null hypothesis*

Suppose that instead of the above, we now want to test the following null hypothesis
$$ \beta_1 + \beta_2 = 0$$ 
against its two-sided alternative
$$ \beta_1 + \beta_2 \neq 0$$ 


We are here testing whether the sum of the parameters on `hsGPA` and `ACT` is statistically significantly different from zero or not. We can do this in R directly by running the following command 

```{r}
# F-test for sum of parameters
linearHypothesis(multivariate_OLS, c("hsGPA + ACT = 0"))
```

As above, we use the `linearHypothesis()` function in R to conduct a *F-test*. While the first argument is again the same as above, the second argument looks a bit different here since we want to test a different hypothesis. Specifically, we want to test whether $\beta_1 + \beta_2 = 0$, which is expressed as the variable names associated to these parameters in R. Note that you could rewrite this command as follows to get the same result

```{r, eval=FALSE}
# F-test for sum of parameters (alternative way)
linearHypothesis(multivariate_OLS, c("hsGPA =- ACT "))
```

Either command will yield an *F-statistic* of 22.258.

We have therefore seen that we can use *F-* and *t-tests* to test single hypotheses. When testing multiple hypotheses, however, you will have to rely on only the *F-test*. The *t-test* will not be applicable in that case. 

## Testing Multiple Hypotheses

Now, suppose we want to test the following null hyoptheses
$$\beta_1 = \beta_2 = \beta_3 = 0$$
In words, we are testing whether all our coefficients (except the intercept) are jointly zero. Be aware that we are now testing multiple hypotheses (3 to be precise). 

```{r}
# testing multiple hypotheses
linearHypothesis(multivariate_OLS, c("hsGPA=0", "ACT=0", "skipped=0"))
```

Look at the summary of our original regression again by typing 
```{r}
# summary statistics of original regression
summary(multivariate_OLS)
```
Notice that the last line provides you with an *F-statistic* and a *p-value* and notice that these are equivalent to the *F-statistic* and *p-value* we just calculated. Hence, if you want to test the null hypotheses that all your parameters are zero (except the intercept), you can rely on the summary output of our regression without having to compute the *F-statistic*. However, if you wanted to test whether $\beta_1$ and $\beta_2$ are jointly zero only (i.e. omitting $\beta_3$ in our example above), then you cannot rely on this output and have to compute the *F-statistic* yourself. 





