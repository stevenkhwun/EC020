---
title: 'Heteroskedasticity'
date: "2023/6/22"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


> Learning points:
>
> * calculate heteroskedasticity robust standard errors using `coeftest()` function
> * F-test under heteroskedasticity assumption using `linearHypothesis()` function
> * use `ggplot()` function of the `ggplot2` package



# Wage Equation Set-Up

We aim to look at the relationship between log wages and education and experience. Mathematically, 
$$log(wage_i) = \alpha_0 + \alpha_1 educ_i + \alpha_2 exper_i + \epsilon_i$$
```{r}
# load the data
library(wooldridge)
data("wage2")
dim(wage2)
```

# OLS with Heteroskedasticity Robust Standard Errors

```{r}
# run the regression
wage2_ols1 <- lm(lwage ~ educ + + exper, data=wage2)
summary(wage2_ols1)     # homosk. SEs
```

The standard errors R reports using the `summary()` command are calculated under the assumption of homoskedasticity.

The assumption of homoskedasiticty is often not satisfied. Indeed, we might want to report heteroskedasiticty robust standard errors. It turns out, that doing this in R is very easy. The regression we run using the `lm()` command stays the same. Then, when displaying the results, instead of using the `summary()` command, we use the `coeftest()` command as follows (make sure you have installed the packages `lmtest` and `sandwich` for this).

```{r message=FALSE}
# calculate heteroskedasticity robust s.e.
library(lmtest)      # load the required package
library(sandwich)    # load the required package
coeftest(wage2_ols1, vcov=vcovHC(wage2_ols1, type = "HC1"))   # heterosk. SEs
```

The first input into the `coeftest()` function is the object we have estimated above. The second one is a bit more complicated to when not using matrix notation. Technically, we want to specify the covariance matrix of the estimated coefficients. We do this by using the `vcovHC()` function. This function consistently estimates the covariance matrix of the coefficients of the `wage2_ols1` regression. The "type" arguments specified the "type of heteroskedasticity robust standard errors" we want. This last part is not of great importance. While the inputs into the `coeftest()` function are a bit more complicated, the above code can just be copied to any other regression you ran to then calculate the heteroskedasticity robust standard errors. Furthermore, as you can see from the output, the interpretation and layout of the output is the same as what we're already used to.

The *F-test* is not provided in this output. Luckily, we can still use the `linearHypothesis()` command to report a *F-test* that is robust to heteroskedasticity. Just for illustration purposes, suppose we add a few additional controls into our wage regression and want to test whether all the controls on the right-hand side are jointly zero (assuming heteroskedasticity). In R, we could type

```{r}
# run the regression
wage2_ols2<- lm(lwage ~ educ + exper + age + married + black + 
                  south, data=wage2)
```

```{r message=FALSE}
# F-test assuming heteroskedasticity
library(car)
linearHypothesis(wage2_ols2, c("exper", "age", "married", "black", "south"), 
                 c(0, 0, 0, 0, 0), vcov=hccm(wage2_ols2, type="hc1"))
```

The regression with added control is standard. Then, to conduct the *F-test*, we use the `linearHypothesis()` command. Everything is as we're used to, except that the last argument into the function is once again asking us to input the covariance matrix of the coefficients. This time, we get this covariance matrix using the `hccm()` function. This is just an alternative function to the `vocHC()` function we used above. You can convince yourself of this by typing 

```{r eval=FALSE}
# F-test assuming heteroskedasticity (alternative coding)
linearHypothesis(wage2_ols2, c("exper", "age", "married", "black", "south"), 
                 c(0, 0, 0, 0, 0), vcov=vcovHC(wage2_ols2, type="HC1")) # F-test
```
The output is exactly the same. 

# Exploring Heteroskedasticity

We will consider three ways of exploring heteroskedasticity in more detail. The first is to plot our data. The second and third are to test for heteroskedasticity using the Breusch-Pagan and White tests, respectively.

## Visual Inspection

To visually inspect whether we should be concerned about heteroskedasticity, we could plot the residuals squared against either the predicted values of the outcome or against any of our regressors of interest.

```{r}
# plot the residuals against the predicted values
wage2$ols1resid <- resid(wage2_ols1)            # get residuals
wage2$fitted1wage <- fitted.values(wage2_ols1)  # get fitted values
plot(wage2$fitted1wage, wage2$ols1resid, xlab="Fitted Log Wage", ylab="Residuals")
```

```{r}
# plot the residuals square against the predicted values
wage2$ols1residsq <- wage2$ols1resid^2          # square the residuals
plot(wage2$fitted1wage, wage2$ols1residsq, xlab="Fitted Log Wage", ylab="Residuals Square")
```

The plot of the fitted wages against the residuals does not necessarily suggest that heteroskedasticity is a huge issue.

Other ways to create plots in R:
```{r}
# plot the residuals against the predicted values using ggplot()
library(ggplot2)
ggplot(wage2, aes(x=fitted1wage, y=ols1resid)) + geom_point()
```

## Breusch-Pagan Test


The Breusch-Pagan test provides one way to test heteroskedasticity more formally. The implementation of this test consists of three steps. The first step is to run the regression of interest and to then compute the residuals squared of said regression. Notice that we have already done this above. The residuals squared in our case are called $ols1residsq$. The second step is to regress these squared residuals on all explanatory variables (education and experience in our case) and to compute the usual F-statistic. The third step is to then look at the output of this test and to conclude whether we have a heteroskedasticity issue or not. The null hypothesis is that our assumption of homoskedasticity holds (i.e. that the coefficients on education and experience are jointly zero). In R, implementing step 2 is simple, i.e.

```{r}
# Breusch-Pagan Test step 2
BP_step2 <- lm(ols1residsq ~ educ + exper, data=wage2)
summary(BP_step2)
```

The resulting F-statistic is $2.042$ with a p-value of $0.1303$, implying that we fail to reject the null hypothesis of homoskedasticity. Hence, the Breusch-Pagan test confirms what we suspected when looking at the data visually: we do not seem to have a heteroskedasticity problem.


## White Test

There are two cases of the White test. The first and general case regresses the residuals squared on education, experience, as well as the square of each term plus their interaction. The second more special case of the White test regresses the residuals squared on the fitted values of log wages as well as the square of these fitted values. Otherwise, the test procedure is analogous to the Breusch-Pagan test discussed above. In R, this is all very straightforward, i.e. 

```{r}
# define squares and interactions for white test
wage2$educsq <- wage2$educ^2              # educ squared
wage2$expersq <- wage2$exper^2            # exper squared
wage2$educexper <- wage2$educ*wage2$exper # educ exper interaction
```

```{r}
# define fitted values and squares for the special white test
wage2$olsfitted <- fitted.values(wage2_ols1)
wage2$olsfittedsq <- wage2$olsfitted^2
```

```{r}
# Usual White test
White_1 <- lm(ols1residsq ~ educ + exper + educsq + expersq + educexper,
              data=wage2)
summary(White_1)
```

```{r}
# Special White test
White_2 <- lm(ols1residsq ~ olsfitted + olsfittedsq, data=wage2)
summary(White_2)
```

The resulting F-statistics are $0.8436$ and $.02164$ with p-values of $0.5189$ and $0.8055$, respectively for the usual and special White test. Clearly, we again fail to reject the null hypothesis of homoskedasticity. Thus, the White test confirms that heteroskedasticity does not seem to pose an issue in our data. 

