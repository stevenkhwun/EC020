---
title: 'Linear Regression: Estimation'
date: "2023/6/22"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


> Learning points:
>
> * read csv file using `read.csv()`
> * run OLS regression using `lm()`
> * create plots using `plot()`
> * read dta file using `read.dta()` in library `foreign`
> * subset data using `subset()`
> * the option of `na.action = na.exclude` in the `lm()` function


# Running an OLS Regression

```{r}
# load csv data (library foreign not required)
exp <- read.csv("Activity1/lalondeexp.csv")
head(exp)
```

```{r}
# run OLS regression
OLS <- lm(re78 ~ treat, data=exp)
summary(OLS)
```

Note that R automatically includes an intercept. If you want to run an OLS without the intercept, add `- 1` at the end of the equation, just like:

```{r}
# run OLS regression without intercept
OLS_no_intercept <- lm(re78 ~ treat - 1, data =exp)
summary(OLS_no_intercept)
```


```{r}
# get coefficients
coefficients <- coef(OLS)
coefficients
# get residuals
residuals <- residuals(OLS)
head(residuals)
# get fitted values
fitted_values <- fitted.values(OLS)
head(fitted_values)
```

```{r}
# plot the residuals against treatment variable
plot(exp$treat, residuals,
     xlab = "Treatment", ylab = "Residuals",
     main = "Residuals vs. Treatment")
```

```{r}
# plot the residuals against fitted values
plot(fitted_values, residuals,
     xlab = "Fitted values", ylab = "Residuals",
     main = "Residuals vs. Fitted Values")
```

# Multiple Regression Model: Estimation

```{r}
# Load the required package
library(foreign)
```

```{r, warning=FALSE}
# Read the data
data <- read.dta("Activity2/CARD.dta")
```

The data has 34 variables and 3010 observations. We only care about a few of these, namely "wage", "educ", "IQ", "black", and "exper".

```{r}
data_for_analysis <- subset(data, select = c(wage, educ, IQ, black, exper)) 
head(data_for_analysis)
```

```{r}
# Create log wages
data_for_analysis$lwage <- log(data_for_analysis$wage)
```

## Bivariate Regression

$$ log(wage)_i = \beta_0 + \beta_1 educ_i + \epsilon_i $$

```{r}
# Run OLS regression
bivariate_OLS <- lm(lwage ~ educ, data=data_for_analysis)
summary(bivariate_OLS)
```

The estimated coefficient on education is 0.052 implies that an additional year of schooling is associated with a 5.2 percent increase in wages. 

```{r}
# plot residuals against education
residual <- residuals(bivariate_OLS)    # get the residuals
plot(data_for_analysis$educ, residual,
     xlab = "Years of education", ylab = "Residuals",
     main = "Residuals vs. Years of education")
```

```{r}
# get R squared
summary(bivariate_OLS)$r.squared
```

```{r}
# get adjusted R squared
summary(bivariate_OLS)$adj.r.squared
```

## Multivariate Regressions

*Adding one control*

$$ log(wage)_i = \gamma_0 + \gamma_1 educ_i + \gamma_2 exper_i  + v_i $$

```{r}
# run multiple OLS regression
multivar_OLS <- lm(lwage ~ educ + exper, data=data_for_analysis)
summary(multivar_OLS)
```

### Omitted variable bias formula

The fact that the coefficient on education changes when we include work experience as a control variable leads us to conclude that the estimate on education in the bivariate regression was (downward) biased. To see why this is, recall the **omitted variable bias formula**:

$$ \hat{\beta}_1 = \hat{\gamma}_1 + \hat{\gamma}_2 \hat{\pi}$$

where $\hat{\pi}$ is the OLS estimate from a regression of $exper$ on $educ$. The bivariate and multivariate regressions we ran provide us with three of these coefficients, i.e.
$$.052 = .093 + .041 \hat{\pi}$$

Solving for $\hat{\pi}$ we get $\hat{\pi} = -1$. This implies, that in a regression of $exper$ on $edu$, the coefficient on $educ$ should be $-1$. We can test this by running the following auxilliary regression

```{r}
# running auxilliary regression
aux_reg <- lm(exper ~ educ, data=data_for_analysis)
summary(aux_reg)
```

*Adding a second control: IQ*

$$ log(wage)_i = \gamma_0 + \gamma_1 educ_i + \gamma_2 IQ_i + \gamma_3 exper_i + v_i $$

```{r}
# IQ has missing value
summary(data_for_analysis$IQ)
```

We include the option of `na.action = na.exclude` in the `lm()` function to exclude the missing value. In fact, `R` automatically drops all missing observations in a regression. However, it is good practice to include option as it will force you to think about what observations you drop from your sample an possible selection issues.

```{r}
# runing second multiple OLS regression
multivariate_OLS <- lm(lwage ~ educ + IQ + exper, data=data_for_analysis,
                       na.action=na.exclude)
summary(multivariate_OLS)
```

```{r}
# get R squared
summary(multivariate_OLS)$r.squared
# get adjusted R squared
summary(multivariate_OLS)$adj.r.squared
```

### Frisch-Waugh theorem

The Frisch-Waugh theorem says that whether we run the multivariate regression as above leads to the same coefficient on education as if we (i) run an OLS regression of education on our controls and (ii) run an OLS regression of log wages on the residual obtained from the regression in (i). We already ran the multivariate regression above, so we can here implement the two step procedure to see if we get the same coefficient on education. To implement the procedure, run the following code:

```{r}
# first step OLs
step1 <- lm(educ ~ IQ + exper, data=data_for_analysis, na.action=na.exclude)
# get residuals for step 1
data_for_analysis$step1_residuals <- residuals(step1)
# second step OLS
step2 <- lm(lwage ~ step1_residuals, data=data_for_analysis, na.action=na.exclude)
summary(step2)
```

## Multicollinearity

We extend the model to include a dummy variable indicating whether an individual is black or not.

$$ log(wage)_i = \gamma_0 + \gamma_1 educ_i + \gamma_2 IQ_i + \gamma_3 exper_i + \gamma_4 black_i + v_i $$
```{r}
# run OLS regression
multivariate_OLS2 <- lm(lwage ~ educ + IQ + exper + black, data=data_for_analysis,
                        na.action=na.exclude)
summary(multivariate_OLS2)
```

Now create a new variable called `nblack`, which is "the opposit" of the variable `black`.

```{r}
# create the variabel nblack
data_for_analysis$nblack = 1 - data_for_analysis$black
```

```{r}
# run an extreme case of multicollinearity
multivariate_OLS3 <- lm(lwage ~ educ + IQ + exper + black + nblack, data=data_for_analysis,
                        na.action=na.exclude)
summary(multivariate_OLS3)
```

For perfect multicollinearity, R automatically omits the new regressor.

However, if we were to drop the intercept from the regression, R will estimate a coefficient for both variables.

```{r}
# run an extreme case of multicollinearity
multivariate_OLS3 <- lm(lwage ~ educ + IQ + exper + black + nblack - 1,
                        data=data_for_analysis, na.action=na.exclude)
summary(multivariate_OLS3)
```
